{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marumarukun/pj/compe/atma_18/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_number: '002'\n",
      "run_time: base\n",
      "data:\n",
      "  input_root: ../../data/input\n",
      "  train_path: ../../data/input/train_features.csv\n",
      "  test_path: ../../data/input/test_features.csv\n",
      "  sample_submission_path: ../../data/input/sample_submission.csv\n",
      "  img_root: ../../data/input/images\n",
      "  output_root: ../../data/output\n",
      "  results_root: ../../results\n",
      "  results_path: ../../results/002/base\n",
      "seed: 319\n",
      "n_splits: 5\n",
      "target_cols:\n",
      "- x_0\n",
      "- y_0\n",
      "- z_0\n",
      "- x_1\n",
      "- y_1\n",
      "- z_1\n",
      "- x_2\n",
      "- y_2\n",
      "- z_2\n",
      "- x_3\n",
      "- y_3\n",
      "- z_3\n",
      "- x_4\n",
      "- y_4\n",
      "- z_4\n",
      "- x_5\n",
      "- y_5\n",
      "- z_5\n",
      "cnn:\n",
      "  model_name: tf_efficientnet_b0_ns\n",
      "  size: 224\n",
      "  pretrained: true\n",
      "  in_chans: 9\n",
      "  target_size: 18\n",
      "  lr: 0.001\n",
      "  num_epochs: 20\n",
      "  batch_size: 64\n",
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pprint\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pytz\n",
    "import timm\n",
    "import torch\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from src.config import cfg\n",
    "from src.dir import create_dir\n",
    "from src.seed import seed_everything\n",
    "\n",
    "cfg.exp_number = Path().resolve().name\n",
    "print(OmegaConf.to_yaml(cfg, resolve=True))\n",
    "\n",
    "seed_everything(cfg.seed)\n",
    "pl.Config.set_fmt_str_lengths(1000)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exp002\n",
    "\n",
    "- NNモデルを作ってみるnotebook\n",
    "- とりあえずCNNで特徴抽出 → 全結合層で回帰というシンプルなアーキテクチャを採用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "train = pl.read_csv(cfg.data.train_path, try_parse_dates=True)\n",
    "test = pl.read_csv(cfg.data.test_path, try_parse_dates=True)\n",
    "sample_submission = pl.read_csv(cfg.data.sample_submission_path, try_parse_dates=True)\n",
    "\n",
    "# データの結合(label encoding用)\n",
    "train_test = pl.concat([train, test], how=\"diagonal\")\n",
    "\n",
    "# scene列を作成 → これでGroupKFoldする\n",
    "train = train.with_columns(pl.col(\"ID\").str.split(\"_\").list[0].alias(\"scene\"))\n",
    "test = test.with_columns(pl.col(\"ID\").str.split(\"_\").list[0].alias(\"scene\"))\n",
    "\n",
    "# CV\n",
    "gkf = GroupKFold(n_splits=cfg.n_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 31)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ID</th><th>vEgo</th><th>aEgo</th><th>steeringAngleDeg</th><th>steeringTorque</th><th>brake</th><th>brakePressed</th><th>gas</th><th>gasPressed</th><th>gearShifter</th><th>leftBlinker</th><th>rightBlinker</th><th>x_0</th><th>y_0</th><th>z_0</th><th>x_1</th><th>y_1</th><th>z_1</th><th>x_2</th><th>y_2</th><th>z_2</th><th>x_3</th><th>y_3</th><th>z_3</th><th>x_4</th><th>y_4</th><th>z_4</th><th>x_5</th><th>y_5</th><th>z_5</th><th>scene</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>bool</td><td>f64</td><td>bool</td><td>str</td><td>bool</td><td>bool</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>&quot;00066be8e20318869c38c66be466631a_320&quot;</td><td>5.701526</td><td>1.538456</td><td>-2.165777</td><td>-139.0</td><td>0.0</td><td>false</td><td>0.25</td><td>true</td><td>&quot;drive&quot;</td><td>false</td><td>false</td><td>2.82959</td><td>0.032226</td><td>0.045187</td><td>6.231999</td><td>0.065895</td><td>0.107974</td><td>9.785009</td><td>0.124972</td><td>0.203649</td><td>13.485472</td><td>0.163448</td><td>0.302818</td><td>17.574227</td><td>0.174289</td><td>0.406331</td><td>21.951269</td><td>0.199503</td><td>0.485079</td><td>&quot;00066be8e20318869c38c66be466631a&quot;</td></tr><tr><td>&quot;00066be8e20318869c38c66be466631a_420&quot;</td><td>11.176292</td><td>0.279881</td><td>-11.625697</td><td>-44.0</td><td>0.0</td><td>false</td><td>0.0</td><td>false</td><td>&quot;drive&quot;</td><td>false</td><td>true</td><td>4.970268</td><td>-0.007936</td><td>0.005028</td><td>10.350489</td><td>-0.032374</td><td>-0.020701</td><td>15.770054</td><td>0.084073</td><td>0.008645</td><td>21.132415</td><td>0.391343</td><td>0.036335</td><td>26.316489</td><td>0.843124</td><td>0.065</td><td>31.383814</td><td>1.42507</td><td>0.073083</td><td>&quot;00066be8e20318869c38c66be466631a&quot;</td></tr><tr><td>&quot;00066be8e20318869c38c66be466631a_520&quot;</td><td>10.472548</td><td>0.231099</td><td>-2.985105</td><td>-132.0</td><td>0.0</td><td>false</td><td>0.18</td><td>true</td><td>&quot;drive&quot;</td><td>false</td><td>false</td><td>4.815701</td><td>-0.000813</td><td>0.017577</td><td>10.153522</td><td>-0.0278</td><td>0.026165</td><td>15.446539</td><td>-0.155987</td><td>0.040397</td><td>20.61816</td><td>-0.356932</td><td>0.058765</td><td>25.677387</td><td>-0.576985</td><td>0.102859</td><td>30.460033</td><td>-0.841894</td><td>0.152889</td><td>&quot;00066be8e20318869c38c66be466631a&quot;</td></tr><tr><td>&quot;000fb056f97572d384bae4f5fc1e0f28_120&quot;</td><td>6.055565</td><td>-0.117775</td><td>7.632668</td><td>173.0</td><td>0.0</td><td>false</td><td>0.0</td><td>false</td><td>&quot;drive&quot;</td><td>false</td><td>false</td><td>2.812608</td><td>0.033731</td><td>0.0059</td><td>5.975378</td><td>0.137848</td><td>0.01621</td><td>9.186793</td><td>0.322997</td><td>0.031626</td><td>12.37311</td><td>0.603145</td><td>0.031858</td><td>15.703514</td><td>0.960717</td><td>0.043479</td><td>19.311182</td><td>1.374655</td><td>0.058754</td><td>&quot;000fb056f97572d384bae4f5fc1e0f28&quot;</td></tr><tr><td>&quot;000fb056f97572d384bae4f5fc1e0f28_20&quot;</td><td>3.316744</td><td>1.276733</td><td>-31.725477</td><td>-114.0</td><td>0.0</td><td>false</td><td>0.255</td><td>true</td><td>&quot;drive&quot;</td><td>false</td><td>false</td><td>1.55186</td><td>-0.041849</td><td>-0.008847</td><td>3.675162</td><td>-0.125189</td><td>-0.013725</td><td>6.113567</td><td>-0.239161</td><td>-0.012887</td><td>8.770783</td><td>-0.381813</td><td>-0.003898</td><td>11.619313</td><td>-0.554488</td><td>0.011393</td><td>14.657048</td><td>-0.7788</td><td>0.044243</td><td>&quot;000fb056f97572d384bae4f5fc1e0f28&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 31)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬──────────┬───────────┐\n",
       "│ ID        ┆ vEgo      ┆ aEgo      ┆ steeringA ┆ … ┆ x_5       ┆ y_5       ┆ z_5      ┆ scene     │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ngleDeg   ┆   ┆ ---       ┆ ---       ┆ ---      ┆ ---       │\n",
       "│ str       ┆ f64       ┆ f64       ┆ ---       ┆   ┆ f64       ┆ f64       ┆ f64      ┆ str       │\n",
       "│           ┆           ┆           ┆ f64       ┆   ┆           ┆           ┆          ┆           │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪══════════╪═══════════╡\n",
       "│ 00066be8e ┆ 5.701526  ┆ 1.538456  ┆ -2.165777 ┆ … ┆ 21.951269 ┆ 0.199503  ┆ 0.485079 ┆ 00066be8e │\n",
       "│ 20318869c ┆           ┆           ┆           ┆   ┆           ┆           ┆          ┆ 20318869c │\n",
       "│ 38c66be46 ┆           ┆           ┆           ┆   ┆           ┆           ┆          ┆ 38c66be46 │\n",
       "│ 6631a_320 ┆           ┆           ┆           ┆   ┆           ┆           ┆          ┆ 6631a     │\n",
       "│ 00066be8e ┆ 11.176292 ┆ 0.279881  ┆ -11.62569 ┆ … ┆ 31.383814 ┆ 1.42507   ┆ 0.073083 ┆ 00066be8e │\n",
       "│ 20318869c ┆           ┆           ┆ 7         ┆   ┆           ┆           ┆          ┆ 20318869c │\n",
       "│ 38c66be46 ┆           ┆           ┆           ┆   ┆           ┆           ┆          ┆ 38c66be46 │\n",
       "│ 6631a_420 ┆           ┆           ┆           ┆   ┆           ┆           ┆          ┆ 6631a     │\n",
       "│ 00066be8e ┆ 10.472548 ┆ 0.231099  ┆ -2.985105 ┆ … ┆ 30.460033 ┆ -0.841894 ┆ 0.152889 ┆ 00066be8e │\n",
       "│ 20318869c ┆           ┆           ┆           ┆   ┆           ┆           ┆          ┆ 20318869c │\n",
       "│ 38c66be46 ┆           ┆           ┆           ┆   ┆           ┆           ┆          ┆ 38c66be46 │\n",
       "│ 6631a_520 ┆           ┆           ┆           ┆   ┆           ┆           ┆          ┆ 6631a     │\n",
       "│ 000fb056f ┆ 6.055565  ┆ -0.117775 ┆ 7.632668  ┆ … ┆ 19.311182 ┆ 1.374655  ┆ 0.058754 ┆ 000fb056f │\n",
       "│ 97572d384 ┆           ┆           ┆           ┆   ┆           ┆           ┆          ┆ 97572d384 │\n",
       "│ bae4f5fc1 ┆           ┆           ┆           ┆   ┆           ┆           ┆          ┆ bae4f5fc1 │\n",
       "│ e0f28_120 ┆           ┆           ┆           ┆   ┆           ┆           ┆          ┆ e0f28     │\n",
       "│ 000fb056f ┆ 3.316744  ┆ 1.276733  ┆ -31.72547 ┆ … ┆ 14.657048 ┆ -0.7788   ┆ 0.044243 ┆ 000fb056f │\n",
       "│ 97572d384 ┆           ┆           ┆ 7         ┆   ┆           ┆           ┆          ┆ 97572d384 │\n",
       "│ bae4f5fc1 ┆           ┆           ┆           ┆   ┆           ┆           ┆          ┆ bae4f5fc1 │\n",
       "│ e0f28_20  ┆           ┆           ┆           ┆   ┆           ┆           ┆          ┆ e0f28     │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴──────────┴───────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ拡張\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(cfg.cnn.size, cfg.cnn.size),\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    A.GaussNoise(var_limit=[10, 50]),\n",
    "                    A.GaussianBlur(),\n",
    "                    A.MotionBlur(),\n",
    "                ],\n",
    "                p=0.4,\n",
    "            ),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(cfg.cnn.size, cfg.cnn.size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = Path(img_dir)\n",
    "\n",
    "        # デフォルトの変換処理\n",
    "        if transform is None:\n",
    "            self.transform = A.Compose(\n",
    "                [\n",
    "                    A.Resize(cfg.cnn.size, cfg.cnn.size),\n",
    "                    A.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406],  # 通常のImageNet平均値\n",
    "                        std=[0.229, 0.224, 0.225],  # 通常のImageNet標準偏差\n",
    "                    ),\n",
    "                    ToTensorV2(),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        self.target_cols = cfg.target_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df[idx]\n",
    "        img_folder = self.img_dir / row[\"ID\"].item()\n",
    "\n",
    "        # 3枚の画像を読み込み、変換を適用\n",
    "        img_names = [\"image_t-1.0.png\", \"image_t-0.5.png\", \"image_t.png\"]\n",
    "        transformed_imgs = []\n",
    "\n",
    "        for img_name in img_names:\n",
    "            img_path = img_folder / img_name\n",
    "            img = cv2.imread(str(img_path))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            if self.transform:\n",
    "                transformed = self.transform(image=img)\n",
    "                transformed_imgs.append(transformed[\"image\"])\n",
    "\n",
    "        # チャネル方向に結合 (C*3, H, W)\n",
    "        img_tensor = torch.cat(transformed_imgs, dim=0)\n",
    "\n",
    "        # ターゲットの準備\n",
    "        target = torch.tensor(row[self.target_cols].to_numpy(), dtype=torch.float32).squeeze(0)\n",
    "\n",
    "        return img_tensor, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, pretrained=False, target_size=None, model_name=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            cfg.model_name, pretrained=cfg.pretrained, num_classes=0, in_chans=cfg.in_chans\n",
    "        )\n",
    "\n",
    "        self.n_features = self.encoder.num_features\n",
    "\n",
    "        self.target_size = cfg.target_size if target_size is None else target_size\n",
    "\n",
    "        # nn.Dropout(0.5),\n",
    "        self.fc = nn.Sequential(nn.Linear(self.n_features, self.target_size))\n",
    "\n",
    "    def forward(self, image):\n",
    "        feature = self.encoder(image)\n",
    "        output = self.fc(feature)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marumarukun/pj/compe/atma_18/.venv/lib/python3.12/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "model = CustomModel(cfg.cnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created: ../../results/002/20241119_201834\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marumarukun/pj/compe/atma_18/.venv/lib/python3.12/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n",
      "100%|██████████| 543/543 [03:41<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Valid Loss: 0.9250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 543/543 [03:38<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Valid Loss: 0.9834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 543/543 [03:41<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Valid Loss: 0.7513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 543/543 [03:44<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Valid Loss: 0.7217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 543/543 [03:40<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Valid Loss: 0.6727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 543/543 [03:41<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Valid Loss: 0.6522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 543/543 [03:43<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Valid Loss: 0.6143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 543/543 [03:43<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Valid Loss: 0.6058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 543/543 [03:43<00:00,  2.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# 実験結果格納用のディレクトリを作成\n",
    "japan_tz = pytz.timezone(\"Asia/Tokyo\")\n",
    "cfg.run_time = datetime.now(japan_tz).strftime(\"%Y%m%d_%H%M%S\")\n",
    "create_dir(cfg.data.results_path)\n",
    "\n",
    "# CV用の配列を初期化\n",
    "oof_predictions = np.zeros((len(train), len(cfg.target_cols)))\n",
    "models = {}\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf.split(train, groups=train[\"scene\"])):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    # データセットの作成\n",
    "    train_dataset = CustomDataset(train[train_idx], cfg.data.img_root, transform=get_train_transform())\n",
    "    valid_dataset = CustomDataset(train[valid_idx], cfg.data.img_root, transform=get_valid_transform())\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.cnn.batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=cfg.cnn.batch_size, shuffle=False)\n",
    "\n",
    "    # モデル、損失関数、オプティマイザーの初期化\n",
    "    model = CustomModel(cfg.cnn).to(device)\n",
    "    criterion = nn.HuberLoss()\n",
    "    # criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.cnn.lr)\n",
    "    total_steps = len(train_loader) * cfg.cnn.num_epochs\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=total_steps * 0.1,\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    # 学習ループ\n",
    "    for epoch in range(cfg.cnn.num_epochs):\n",
    "        model.train()\n",
    "        for images, targets in tqdm(train_loader):\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # 検証\n",
    "        model.eval()\n",
    "        valid_losses = []\n",
    "        with torch.no_grad():\n",
    "            for images, targets in valid_loader:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, targets)\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        print(f\"Epoch {epoch + 1}, Valid Loss: {valid_loss:.4f}\")\n",
    "\n",
    "        # ベストモデルの保存\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f\"{cfg.data.results_path}/model_fold{fold}.pth\")\n",
    "\n",
    "    # ベストモデルでOOF予測\n",
    "    model.load_state_dict(torch.load(f\"{cfg.data.results_path}/model_fold{fold}.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    valid_dataset = CustomDataset(train[valid_idx], cfg.data.img_root, transform=get_valid_transform())\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, batch_size=cfg.cnn.batch_size, shuffle=False, num_workers=cfg.cnn.num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(valid_loader):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            start_idx = i * cfg.cnn.batch_size\n",
    "            end_idx = start_idx + outputs.shape[0]\n",
    "            oof_predictions[valid_idx[start_idx:end_idx]] = outputs.cpu().numpy()\n",
    "\n",
    "# CVスコアの計算（MAEの平均）\n",
    "mae_scores = []\n",
    "for i in range(len(cfg.target_cols)):\n",
    "    mae = np.mean(np.abs(oof_predictions[:, i] - train[cfg.target_cols[i]].to_numpy()))\n",
    "    mae_scores.append(mae)\n",
    "\n",
    "cv_score = np.mean(mae_scores)\n",
    "print(f\"CV Score: {cv_score:.4f}\")\n",
    "\n",
    "# oofを保存\n",
    "np.save(f\"{cfg.data.results_path}/oof_predictions.npy\", oof_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # submission\n",
    "# exprs = [pl.Series(target_preds[i]).alias(target_cols[i]) for i in range(len(target_cols))]\n",
    "# submission = sample_submission.with_columns(exprs)\n",
    "# submission.write_csv(f\"{cfg.data.results_path}/submission.csv\")\n",
    "# submission.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
