{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marumarukun/pj/compe/atma_18/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_number: '012'\n",
      "run_time: base\n",
      "data:\n",
      "  input_root: ../../data/input\n",
      "  train_path: ../../data/input/train_features.csv\n",
      "  test_path: ../../data/input/test_features.csv\n",
      "  sample_submission_path: ../../data/input/sample_submission.csv\n",
      "  img_root: ../../data/input/images\n",
      "  json_root: ../../data/input/traffic_lights\n",
      "  depth_root: ../../data/input/depth\n",
      "  output_root: ../../data/output\n",
      "  results_root: ../../results\n",
      "  results_path: ../../results/012/base\n",
      "seed: 319\n",
      "n_splits: 5\n",
      "target_cols:\n",
      "- x_0\n",
      "- y_0\n",
      "- z_0\n",
      "- x_1\n",
      "- y_1\n",
      "- z_1\n",
      "- x_2\n",
      "- y_2\n",
      "- z_2\n",
      "- x_3\n",
      "- y_3\n",
      "- z_3\n",
      "- x_4\n",
      "- y_4\n",
      "- z_4\n",
      "- x_5\n",
      "- y_5\n",
      "- z_5\n",
      "cnn:\n",
      "  model_name: swin_base_patch4_window7_224.ms_in22k_ft_in1k\n",
      "  size: 224\n",
      "  pretrained: true\n",
      "  in_chans: 20\n",
      "  target_size: 18\n",
      "  lr: 0.001\n",
      "  num_epochs: 6\n",
      "  batch_size: 32\n",
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pytz\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from src.config import cfg\n",
    "from src.dir import create_dir\n",
    "from src.seed import seed_everything\n",
    "\n",
    "cfg.exp_number = Path().resolve().name\n",
    "print(OmegaConf.to_yaml(cfg, resolve=True))\n",
    "\n",
    "seed_everything(cfg.seed)\n",
    "pl.Config.set_fmt_str_lengths(1000)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exp012\n",
    "\n",
    "- late sub\n",
    "- 信号機のマスク情報とdepth画像をチャネル方向に結合して入力とするNN\n",
    "- 数値特徴量も入力とする\n",
    "- 全結合層を4層に\n",
    "- targetを標準化して学習する → 推論時にスケールを戻す\n",
    "- backbone：swin_base_patch4_window12_384.ms_in22k_ft_in1k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "train = pl.read_csv(cfg.data.train_path, try_parse_dates=True).with_row_index()\n",
    "test = pl.read_csv(cfg.data.test_path, try_parse_dates=True).with_row_index()\n",
    "sample_submission = pl.read_csv(cfg.data.sample_submission_path, try_parse_dates=True).with_row_index()\n",
    "\n",
    "\n",
    "# scene列を作成 → これでGroupKFoldする\n",
    "train = train.with_columns(pl.col(\"ID\").str.split(\"_\").list[0].alias(\"scene\"))\n",
    "test = test.with_columns(pl.col(\"ID\").str.split(\"_\").list[0].alias(\"scene\"))\n",
    "\n",
    "# データの結合(label encoding用)\n",
    "train_test = pl.concat([train, test], how=\"diagonal\")\n",
    "\n",
    "# CV\n",
    "gkf = GroupKFold(n_splits=cfg.n_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数値特徴量の前処理\n",
    "numeric_cols = [\"vEgo\", \"aEgo\", \"steeringAngleDeg\", \"steeringTorque\", \"gas\"]\n",
    "\n",
    "# 欠損を埋める\n",
    "train_test = train_test.with_columns([pl.col(col).fill_null(0) for col in numeric_cols])\n",
    "\n",
    "# 全ての数値特徴量を標準化\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(train_test[numeric_cols].to_numpy())\n",
    "\n",
    "# 標準化した値でDataFrameを更新\n",
    "train_test = train_test.with_columns(\n",
    "    [pl.Series(scaled_features[:, i]).alias(col) for i, col in enumerate(numeric_cols)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, testに分ける\n",
    "train = train_test.filter(pl.col(\"x_0\").is_not_null()).sort(\"index\")\n",
    "test = train_test.filter(pl.col(\"x_0\").is_null()).sort(\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targetを標準化\n",
    "target_scaler = StandardScaler()\n",
    "scaled_targets = target_scaler.fit_transform(train[cfg.target_cols].to_numpy())  # これは予測値に対しても使う\n",
    "train_target_scaled = train.with_columns(\n",
    "    [pl.Series(scaled_targets[:, i]).alias(col) for i, col in enumerate(cfg.target_cols)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ拡張\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.ReplayCompose(\n",
    "        [\n",
    "            A.Resize(cfg.cnn.size, cfg.cnn.size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.ReplayCompose(\n",
    "        [\n",
    "            A.Resize(cfg.cnn.size, cfg.cnn.size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信号機マスク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficLightMaskGenerator:\n",
    "    TRAFFIC_LIGHT_CLASSES = [\"green\", \"yellow\", \"red\", \"straight\", \"left\", \"right\", \"empty\", \"other\"]\n",
    "\n",
    "    def __init__(self, image_size: list[int, int] | int | None = None):\n",
    "        self.original_size = (128, 64)  # (width, height)\n",
    "\n",
    "        if isinstance(image_size, list):\n",
    "            self.image_size = (image_size[1], image_size[0])\n",
    "        elif isinstance(image_size, int):\n",
    "            self.image_size = (image_size, image_size)\n",
    "        else:\n",
    "            self.image_size = self.original_size\n",
    "\n",
    "        self.scale = (self.image_size[0] / self.original_size[0], self.image_size[1] / self.original_size[1])\n",
    "\n",
    "        self.class_to_index = {cls: idx for idx, cls in enumerate(self.TRAFFIC_LIGHT_CLASSES)}\n",
    "\n",
    "    def scale_bbox(self, bbox):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        return [x1 * self.scale[0], y1 * self.scale[1], x2 * self.scale[0], y2 * self.scale[1]]\n",
    "\n",
    "    def _convert_coordinate(self, coord, max_size):\n",
    "        \"\"\"座標を適切な範囲に収める\"\"\"\n",
    "        return max(0, min(int(round(coord)), max_size - 1))\n",
    "\n",
    "    def generate_masks(self, traffic_lights_json):\n",
    "        # PosixPathオブジェクトを文字列に変換\n",
    "        if isinstance(traffic_lights_json, Path):\n",
    "            traffic_lights_json = str(traffic_lights_json)\n",
    "\n",
    "        traffic_lights = (\n",
    "            json.load(open(traffic_lights_json)) if isinstance(traffic_lights_json, str) else traffic_lights_json\n",
    "        )\n",
    "\n",
    "        masks = np.zeros((self.image_size[1], self.image_size[0], len(self.TRAFFIC_LIGHT_CLASSES)), dtype=np.float32)\n",
    "\n",
    "        for light in traffic_lights:\n",
    "            class_idx = self.class_to_index[light[\"class\"]]\n",
    "            scaled_bbox = self.scale_bbox(light[\"bbox\"])\n",
    "\n",
    "            # 座標を適切な範囲に収める\n",
    "            x1 = self._convert_coordinate(scaled_bbox[0], self.image_size[0])\n",
    "            y1 = self._convert_coordinate(scaled_bbox[1], self.image_size[1])\n",
    "            x2 = self._convert_coordinate(scaled_bbox[2], self.image_size[0])\n",
    "            y2 = self._convert_coordinate(scaled_bbox[3], self.image_size[1])\n",
    "\n",
    "            masks[y1 : y2 + 1, x1 : x2 + 1, class_idx] = 1.0\n",
    "\n",
    "        return masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, json_dir, depth_dir, transform=None, is_train=True):\n",
    "        self.df = df\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.json_dir = Path(json_dir)\n",
    "        self.depth_dir = Path(depth_dir)\n",
    "        self.is_train = is_train\n",
    "        self.feature_cols = [\n",
    "            \"vEgo\",\n",
    "            \"aEgo\",\n",
    "            \"steeringAngleDeg\",\n",
    "            \"steeringTorque\",\n",
    "            \"gas\",\n",
    "            \"leftBlinker\",\n",
    "            \"rightBlinker\",\n",
    "            \"brakePressed\",\n",
    "        ]\n",
    "\n",
    "        # オリジナル画像用の変換処理\n",
    "        if transform is None:\n",
    "            self.transform = A.ReplayCompose(\n",
    "                [\n",
    "                    A.Resize(cfg.cnn.size, cfg.cnn.size),\n",
    "                    A.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406],  # 通常のImageNet平均値\n",
    "                        std=[0.229, 0.224, 0.225],  # 通常のImageNet標準偏差\n",
    "                    ),\n",
    "                    ToTensorV2(),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        # Depth Map用の変換処理\n",
    "        self.depth_transform = A.ReplayCompose(\n",
    "            [\n",
    "                A.Resize(cfg.cnn.size, cfg.cnn.size),\n",
    "                A.Normalize(mean=[0.5], std=[0.5]),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.target_cols = cfg.target_cols\n",
    "        self.mask_generator = TrafficLightMaskGenerator(cfg.cnn.size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df[idx]\n",
    "        img_folder = self.img_dir / row[\"ID\"].item()\n",
    "        depth_folder = self.depth_dir / row[\"ID\"].item()\n",
    "        json_path = self.json_dir / f\"{row['ID'].item()}.json\"\n",
    "\n",
    "        # 3枚の画像を読み込み\n",
    "        img_names = [\"image_t-1.0.png\", \"image_t-0.5.png\", \"image_t.png\"]\n",
    "        imgs = []\n",
    "        depths = []\n",
    "\n",
    "        for img_name in img_names:\n",
    "            # オリジナル画像の読み込み\n",
    "            img_path = img_folder / img_name\n",
    "            img = cv2.imread(str(img_path))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            imgs.append(img)\n",
    "\n",
    "            # Depth Mapの読み込み\n",
    "            depth_path = depth_folder / img_name\n",
    "            depth = cv2.imread(str(depth_path), cv2.IMREAD_GRAYSCALE)\n",
    "            depths.append(depth)\n",
    "\n",
    "        # オリジナル画像の変換\n",
    "        if self.transform:\n",
    "            replay = None\n",
    "            transformed_imgs = []\n",
    "\n",
    "            for img in imgs:\n",
    "                if replay is None:\n",
    "                    # 最初の画像に対して変換を実行し、replayを保存\n",
    "                    transformed = self.transform(image=img)\n",
    "                    replay = transformed[\"replay\"]\n",
    "                else:\n",
    "                    # 2枚目以降は保存したreplayを使用\n",
    "                    transformed = A.ReplayCompose.replay(replay, image=img)\n",
    "                transformed_imgs.append(transformed[\"image\"])\n",
    "\n",
    "        # Depth Mapの変換\n",
    "        transformed_depths = []\n",
    "        for depth in depths:\n",
    "            transformed_depth = self.depth_transform(image=depth)\n",
    "            transformed_depths.append(transformed_depth[\"image\"])\n",
    "\n",
    "        # オリジナル画像をチャネル方向に結合 (C*3, H, W)\n",
    "        img_tensor = torch.cat(transformed_imgs, dim=0)\n",
    "\n",
    "        # Depth Mapをチャネル方向に結合 (3, H, W)\n",
    "        depth_tensor = torch.cat(transformed_depths, dim=0)\n",
    "\n",
    "        # 信号機マスクの生成\n",
    "        mask = self.mask_generator.generate_masks(json_path)\n",
    "        mask_tensor = torch.from_numpy(mask).permute(2, 0, 1)  # (8, H, W)\n",
    "\n",
    "        # 画像, Depth Map, 信号機マスクをチャネル方向に結合 (C*3+3+8=20, H, W)\n",
    "        combined_tensor = torch.cat([img_tensor, depth_tensor, mask_tensor], dim=0)\n",
    "\n",
    "        # 数値特徴量の取得\n",
    "        feature_tensor = torch.tensor(row[self.feature_cols].to_numpy(), dtype=torch.float32).squeeze(0)\n",
    "\n",
    "        # ターゲットの準備\n",
    "        if self.is_train:\n",
    "            target = torch.tensor(row[self.target_cols].to_numpy(), dtype=torch.float32).squeeze(0)\n",
    "            return combined_tensor, feature_tensor, target\n",
    "        else:\n",
    "            return combined_tensor, feature_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, pretrained=False, target_size=None, model_name=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=cfg.pretrained,\n",
    "            num_classes=0,\n",
    "            in_chans=cfg.in_chans,\n",
    "        )\n",
    "\n",
    "        self.n_features = self.encoder.num_features\n",
    "        self.n_numeric_features = 8\n",
    "        self.target_size = cfg.target_size if target_size is None else target_size\n",
    "\n",
    "        # 数値特徴量用のMLP\n",
    "        self.numeric_mlp = nn.Linear(self.n_numeric_features, 128)\n",
    "\n",
    "        # 結合後の特徴量の次元\n",
    "        head_dim = self.n_features + 128\n",
    "\n",
    "        # ヘッド部分を残差接続を使用する形に変更\n",
    "        self.head1 = nn.Linear(head_dim, head_dim)\n",
    "        self.head2 = nn.Linear(head_dim, head_dim)\n",
    "        self.head3 = nn.Linear(head_dim, head_dim)\n",
    "        self.final_layer = nn.Linear(head_dim, self.target_size)\n",
    "\n",
    "    def forward(self, image, numeric_features):\n",
    "        # 画像特徴量の抽出\n",
    "        x = self.encoder(image)\n",
    "\n",
    "        # 数値特徴量の処理\n",
    "        f = self.numeric_mlp(numeric_features)\n",
    "\n",
    "        # 特徴量の結合\n",
    "        x = torch.cat([x, f], dim=-1)\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        # 残差接続を使用したヘッド部分\n",
    "        x_res = F.gelu(self.head1(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = F.gelu(self.head2(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        x_res = F.gelu(self.head3(x))\n",
    "        x = x + x_res\n",
    "\n",
    "        # 最終層\n",
    "        x = self.final_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model test\n",
    "model = CustomModel(cfg.cnn)\n",
    "\n",
    "tmp_image = torch.randn(1, 20, 224, 224)\n",
    "tmp_numeric_features = torch.randn(1, 8)\n",
    "\n",
    "tmp_output = model(tmp_image, tmp_numeric_features)\n",
    "\n",
    "tmp_output.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    cfg.cnn.num_epochs = 1\n",
    "    train = train.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created: ../../results/012/20241125_223138\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [14:58<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Valid Loss: 0.3338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:06<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Valid Loss: 0.3172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:04<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Valid Loss: 0.3076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:09<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Valid Loss: 0.3051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:11<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Valid Loss: 0.3017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:04<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Valid Loss: 0.2989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_229296/869172084.py:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"{cfg.data.results_path}/model_fold{fold}.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:08<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Valid Loss: 0.3315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:05<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Valid Loss: 0.3145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:04<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Valid Loss: 0.3141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:05<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Valid Loss: 0.3048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:06<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Valid Loss: 0.3008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:04<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Valid Loss: 0.2999\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:04<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Valid Loss: 0.3311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:01<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Valid Loss: 0.3256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:00<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Valid Loss: 0.3194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:00<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Valid Loss: 0.3104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [08:59<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Valid Loss: 0.3072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:01<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Valid Loss: 0.3055\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:01<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Valid Loss: 0.3372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [08:58<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Valid Loss: 0.3312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [08:59<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Valid Loss: 0.3235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [08:58<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Valid Loss: 0.3150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:01<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Valid Loss: 0.3106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [08:59<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Valid Loss: 0.3098\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [08:59<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Valid Loss: 0.3304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:08<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Valid Loss: 0.3167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:06<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Valid Loss: 0.3111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:03<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Valid Loss: 0.3098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [09:04<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Valid Loss: 0.3048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1085/1085 [08:59<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Valid Loss: 0.3034\n",
      "CV Score: 0.2317\n"
     ]
    }
   ],
   "source": [
    "# 実験結果格納用のディレクトリを作成\n",
    "japan_tz = pytz.timezone(\"Asia/Tokyo\")\n",
    "cfg.run_time = datetime.now(japan_tz).strftime(\"%Y%m%d_%H%M%S\")\n",
    "create_dir(cfg.data.results_path)\n",
    "\n",
    "# CV用の配列を初期化\n",
    "oof_predictions = np.zeros((len(train), len(cfg.target_cols)))\n",
    "models = {}\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf.split(train, groups=train[\"scene\"])):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    # データセットの作成\n",
    "    train_dataset = CustomDataset(\n",
    "        train_target_scaled[train_idx],\n",
    "        cfg.data.img_root,\n",
    "        cfg.data.json_root,\n",
    "        cfg.data.depth_root,\n",
    "        transform=get_train_transform(),\n",
    "    )\n",
    "    valid_dataset = CustomDataset(\n",
    "        train_target_scaled[valid_idx],\n",
    "        cfg.data.img_root,\n",
    "        cfg.data.json_root,\n",
    "        cfg.data.depth_root,\n",
    "        transform=get_valid_transform(),\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.cnn.batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=cfg.cnn.batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    # モデル、損失関数、オプティマイザーの初期化\n",
    "    model = CustomModel(cfg.cnn).to(device)\n",
    "    # criterion = nn.HuberLoss()\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.cnn.lr)\n",
    "    total_steps = len(train_loader) * cfg.cnn.num_epochs\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=total_steps * 0.1,\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    # 学習ループ\n",
    "    for epoch in range(cfg.cnn.num_epochs):\n",
    "        model.train()\n",
    "        for images, numeric_features, targets in tqdm(train_loader):\n",
    "            images = images.to(device)\n",
    "            numeric_features = numeric_features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, numeric_features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # 検証\n",
    "        model.eval()\n",
    "        valid_losses = []\n",
    "        with torch.no_grad():\n",
    "            for images, numeric_features, targets in valid_loader:\n",
    "                images = images.to(device)\n",
    "                numeric_features = numeric_features.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(images, numeric_features)\n",
    "                loss = criterion(outputs, targets)\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        print(f\"Epoch {epoch + 1}, Valid Loss: {valid_loss:.4f}\")\n",
    "\n",
    "        # ベストモデルの保存\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f\"{cfg.data.results_path}/model_fold{fold}.pth\")\n",
    "\n",
    "    # ベストモデルでOOF予測\n",
    "    model.load_state_dict(torch.load(f\"{cfg.data.results_path}/model_fold{fold}.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    valid_dataset = CustomDataset(\n",
    "        train_target_scaled[valid_idx],\n",
    "        cfg.data.img_root,\n",
    "        cfg.data.json_root,\n",
    "        cfg.data.depth_root,\n",
    "        transform=get_valid_transform(),\n",
    "    )\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=cfg.cnn.batch_size, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, numeric_features, _) in enumerate(valid_loader):\n",
    "            images = images.to(device)\n",
    "            numeric_features = numeric_features.to(device)\n",
    "            outputs = model(images, numeric_features)\n",
    "            start_idx = i * cfg.cnn.batch_size\n",
    "            end_idx = start_idx + outputs.shape[0]\n",
    "            oof_predictions[valid_idx[start_idx:end_idx]] = outputs.cpu().numpy()\n",
    "\n",
    "# CVスコアの計算（MAEの平均）\n",
    "\n",
    "# oofのスケールを戻す\n",
    "oof_predictions = target_scaler.inverse_transform(oof_predictions)\n",
    "mae_scores = []\n",
    "for i in range(len(cfg.target_cols)):\n",
    "    mae = np.mean(np.abs(oof_predictions[:, i] - train[cfg.target_cols[i]].to_numpy()))\n",
    "    mae_scores.append(mae)\n",
    "\n",
    "cv_score = np.mean(mae_scores)\n",
    "print(f\"CV Score: {cv_score:.4f}\")\n",
    "\n",
    "# oofを保存\n",
    "np.save(f\"{cfg.data.results_path}/oof_predictions.npy\", oof_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using fold 1 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_229296/1161565385.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"{cfg.data.results_path}/model_fold{fold}.pth\"))\n",
      "100%|██████████| 54/54 [00:20<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using fold 2 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:18<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using fold 3 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:18<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using fold 4 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:18<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using fold 5 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:18<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 18)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>x_0</th><th>y_0</th><th>z_0</th><th>x_1</th><th>y_1</th><th>z_1</th><th>x_2</th><th>y_2</th><th>z_2</th><th>x_3</th><th>y_3</th><th>z_3</th><th>x_4</th><th>y_4</th><th>z_4</th><th>x_5</th><th>y_5</th><th>z_5</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1.495188</td><td>-0.039181</td><td>-0.000726</td><td>3.143584</td><td>-0.087143</td><td>-0.001501</td><td>4.791918</td><td>-0.135711</td><td>-0.00212</td><td>6.442691</td><td>-0.176083</td><td>-0.000398</td><td>8.087469</td><td>-0.210921</td><td>0.000257</td><td>9.663422</td><td>-0.242781</td><td>0.00303</td></tr><tr><td>0.954703</td><td>0.311374</td><td>-0.003814</td><td>1.851908</td><td>0.817911</td><td>-0.005987</td><td>2.590929</td><td>1.460414</td><td>-0.007047</td><td>3.256482</td><td>2.170239</td><td>-0.01342</td><td>3.835674</td><td>2.946243</td><td>-0.010635</td><td>4.192871</td><td>3.748976</td><td>-0.004953</td></tr><tr><td>1.592748</td><td>0.016054</td><td>0.00036</td><td>3.253291</td><td>0.030881</td><td>0.000265</td><td>4.779555</td><td>0.040324</td><td>-0.000028</td><td>6.171623</td><td>0.0457</td><td>-0.000503</td><td>7.418311</td><td>0.046988</td><td>-0.003007</td><td>8.543471</td><td>0.051794</td><td>-0.005058</td></tr><tr><td>0.88163</td><td>0.046782</td><td>-0.00312</td><td>1.753022</td><td>0.162773</td><td>-0.005505</td><td>2.4879</td><td>0.366899</td><td>-0.008358</td><td>3.053479</td><td>0.664821</td><td>-0.013423</td><td>3.520385</td><td>1.04099</td><td>-0.020913</td><td>3.807704</td><td>1.474138</td><td>-0.032787</td></tr><tr><td>0.819554</td><td>0.008511</td><td>-0.009766</td><td>1.403461</td><td>0.015606</td><td>-0.02418</td><td>1.75973</td><td>0.01892</td><td>-0.041164</td><td>1.955308</td><td>0.019548</td><td>-0.057974</td><td>2.017004</td><td>0.013897</td><td>-0.074355</td><td>2.108573</td><td>0.011606</td><td>-0.088628</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 18)\n",
       "┌──────────┬───────────┬───────────┬──────────┬───┬───────────┬──────────┬───────────┬───────────┐\n",
       "│ x_0      ┆ y_0       ┆ z_0       ┆ x_1      ┆ … ┆ z_4       ┆ x_5      ┆ y_5       ┆ z_5       │\n",
       "│ ---      ┆ ---       ┆ ---       ┆ ---      ┆   ┆ ---       ┆ ---      ┆ ---       ┆ ---       │\n",
       "│ f64      ┆ f64       ┆ f64       ┆ f64      ┆   ┆ f64       ┆ f64      ┆ f64       ┆ f64       │\n",
       "╞══════════╪═══════════╪═══════════╪══════════╪═══╪═══════════╪══════════╪═══════════╪═══════════╡\n",
       "│ 1.495188 ┆ -0.039181 ┆ -0.000726 ┆ 3.143584 ┆ … ┆ 0.000257  ┆ 9.663422 ┆ -0.242781 ┆ 0.00303   │\n",
       "│ 0.954703 ┆ 0.311374  ┆ -0.003814 ┆ 1.851908 ┆ … ┆ -0.010635 ┆ 4.192871 ┆ 3.748976  ┆ -0.004953 │\n",
       "│ 1.592748 ┆ 0.016054  ┆ 0.00036   ┆ 3.253291 ┆ … ┆ -0.003007 ┆ 8.543471 ┆ 0.051794  ┆ -0.005058 │\n",
       "│ 0.88163  ┆ 0.046782  ┆ -0.00312  ┆ 1.753022 ┆ … ┆ -0.020913 ┆ 3.807704 ┆ 1.474138  ┆ -0.032787 │\n",
       "│ 0.819554 ┆ 0.008511  ┆ -0.009766 ┆ 1.403461 ┆ … ┆ -0.074355 ┆ 2.108573 ┆ 0.011606  ┆ -0.088628 │\n",
       "└──────────┴───────────┴───────────┴──────────┴───┴───────────┴──────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testの推論\n",
    "test_dataset = CustomDataset(\n",
    "    test, cfg.data.img_root, cfg.data.json_root, cfg.data.depth_root, transform=get_valid_transform(), is_train=False\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.cnn.batch_size, shuffle=False)\n",
    "\n",
    "# fold分の予測値を格納する配列\n",
    "test_predictions = np.zeros((len(test), len(cfg.target_cols), cfg.n_splits))\n",
    "\n",
    "# 各foldのモデルで予測\n",
    "for fold in range(cfg.n_splits):\n",
    "    print(f\"Predicting using fold {fold + 1} model\")\n",
    "\n",
    "    # モデルの読み込み\n",
    "    model = CustomModel(cfg.cnn).to(device)\n",
    "    model.load_state_dict(torch.load(f\"{cfg.data.results_path}/model_fold{fold}.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    fold_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images, numeric_features in tqdm(test_loader):\n",
    "            images = images.to(device)\n",
    "            numeric_features = numeric_features.to(device)\n",
    "            outputs = model(images, numeric_features)\n",
    "            fold_predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "    # バッチごとの予測を結合\n",
    "    fold_predictions = np.concatenate(fold_predictions, axis=0)\n",
    "    test_predictions[:, :, fold] = fold_predictions\n",
    "\n",
    "# fold分の予測値の平均を計算\n",
    "final_predictions = test_predictions.mean(axis=2)\n",
    "\n",
    "# 推論値のスケールを戻す\n",
    "final_predictions = target_scaler.inverse_transform(final_predictions)\n",
    "\n",
    "# submissionファイルの作成\n",
    "exprs = [pl.Series(final_predictions[:, i]).alias(cfg.target_cols[i]) for i in range(len(cfg.target_cols))]\n",
    "submission = sample_submission.with_columns(exprs)\n",
    "submission= submission.drop(\"index\")\n",
    "submission.write_csv(f\"{cfg.data.results_path}/submission.csv\")\n",
    "print(\"Submission file created!\")\n",
    "\n",
    "# 確認\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_predictionsを保存\n",
    "np.save(f\"{cfg.data.results_path}/final_predictions.npy\", final_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1727, 18)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
