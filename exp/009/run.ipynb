{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_number: '009'\n",
      "run_time: base\n",
      "data:\n",
      "  input_root: ../../data/input\n",
      "  train_path: ../../data/input/train_features.csv\n",
      "  test_path: ../../data/input/test_features.csv\n",
      "  sample_submission_path: ../../data/input/sample_submission.csv\n",
      "  img_root: ../../data/input/images\n",
      "  json_root: ../../data/input/traffic_lights\n",
      "  depth_root: ../../data/input/depth\n",
      "  output_root: ../../data/output\n",
      "  results_root: ../../results\n",
      "  results_path: ../../results/009/base\n",
      "seed: 319\n",
      "n_splits: 4\n",
      "target_cols:\n",
      "- x_0\n",
      "- y_0\n",
      "- z_0\n",
      "- x_1\n",
      "- y_1\n",
      "- z_1\n",
      "- x_2\n",
      "- y_2\n",
      "- z_2\n",
      "- x_3\n",
      "- y_3\n",
      "- z_3\n",
      "- x_4\n",
      "- y_4\n",
      "- z_4\n",
      "- x_5\n",
      "- y_5\n",
      "- z_5\n",
      "cnn:\n",
      "  model_name: convnext_base.fb_in22k_ft_in1k\n",
      "  size: 128\n",
      "  pretrained: true\n",
      "  in_chans: 20\n",
      "  target_size: 18\n",
      "  lr: 0.001\n",
      "  num_epochs: 20\n",
      "  batch_size: 32\n",
      "\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pytz\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from src.config import cfg\n",
    "from src.dir import create_dir\n",
    "from src.seed import seed_everything\n",
    "\n",
    "cfg.exp_number = Path().resolve().name\n",
    "print(OmegaConf.to_yaml(cfg, resolve=True))\n",
    "\n",
    "seed_everything(cfg.seed)\n",
    "pl.Config.set_fmt_str_lengths(1000)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exp009\n",
    "\n",
    "- late sub\n",
    "- 信号機のマスク情報とdepth画像をチャネル方向に結合して入力とするNN\n",
    "- 数値特徴量も入力とする\n",
    "- backbone：convnext_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "train = pl.read_csv(cfg.data.train_path, try_parse_dates=True).with_row_index()\n",
    "test = pl.read_csv(cfg.data.test_path, try_parse_dates=True).with_row_index()\n",
    "sample_submission = pl.read_csv(cfg.data.sample_submission_path, try_parse_dates=True).with_row_index()\n",
    "\n",
    "\n",
    "# scene列を作成 → これでGroupKFoldする\n",
    "train = train.with_columns(pl.col(\"ID\").str.split(\"_\").list[0].alias(\"scene\"))\n",
    "test = test.with_columns(pl.col(\"ID\").str.split(\"_\").list[0].alias(\"scene\"))\n",
    "\n",
    "# データの結合(label encoding用)\n",
    "train_test = pl.concat([train, test], how=\"diagonal\")\n",
    "\n",
    "# CV\n",
    "gkf = GroupKFold(n_splits=cfg.n_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数値特徴量の前処理\n",
    "numeric_cols = [\"vEgo\", \"aEgo\", \"steeringAngleDeg\", \"steeringTorque\", \"gas\"]\n",
    "\n",
    "# 欠損を埋める\n",
    "train_test = train_test.with_columns([pl.col(col).fill_null(0) for col in numeric_cols])\n",
    "\n",
    "# 全ての数値特徴量を標準化\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(train_test[numeric_cols].to_numpy())\n",
    "\n",
    "# 標準化した値でDataFrameを更新\n",
    "train_test = train_test.with_columns(\n",
    "    [pl.Series(scaled_features[:, i]).alias(col) for i, col in enumerate(numeric_cols)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, testに分ける\n",
    "train = train_test.filter(pl.col(\"x_0\").is_not_null()).sort(\"index\")\n",
    "test = train_test.filter(pl.col(\"x_0\").is_null()).sort(\"index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ拡張\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.ReplayCompose(\n",
    "        [\n",
    "            A.Resize(cfg.cnn.size, cfg.cnn.size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.ReplayCompose(\n",
    "        [\n",
    "            A.Resize(cfg.cnn.size, cfg.cnn.size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信号機マスク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficLightMaskGenerator:\n",
    "    TRAFFIC_LIGHT_CLASSES = [\"green\", \"yellow\", \"red\", \"straight\", \"left\", \"right\", \"empty\", \"other\"]\n",
    "\n",
    "    def __init__(self, image_size: list[int, int] | int | None = None):\n",
    "        self.original_size = (128, 64)  # (width, height)\n",
    "\n",
    "        if isinstance(image_size, list):\n",
    "            self.image_size = (image_size[1], image_size[0])\n",
    "        elif isinstance(image_size, int):\n",
    "            self.image_size = (image_size, image_size)\n",
    "        else:\n",
    "            self.image_size = self.original_size\n",
    "\n",
    "        self.scale = (self.image_size[0] / self.original_size[0], self.image_size[1] / self.original_size[1])\n",
    "\n",
    "        self.class_to_index = {cls: idx for idx, cls in enumerate(self.TRAFFIC_LIGHT_CLASSES)}\n",
    "\n",
    "    def scale_bbox(self, bbox):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        return [x1 * self.scale[0], y1 * self.scale[1], x2 * self.scale[0], y2 * self.scale[1]]\n",
    "\n",
    "    def _convert_coordinate(self, coord, max_size):\n",
    "        \"\"\"座標を適切な範囲に収める\"\"\"\n",
    "        return max(0, min(int(round(coord)), max_size - 1))\n",
    "\n",
    "    def generate_masks(self, traffic_lights_json):\n",
    "        # PosixPathオブジェクトを文字列に変換\n",
    "        if isinstance(traffic_lights_json, Path):\n",
    "            traffic_lights_json = str(traffic_lights_json)\n",
    "\n",
    "        traffic_lights = (\n",
    "            json.load(open(traffic_lights_json)) if isinstance(traffic_lights_json, str) else traffic_lights_json\n",
    "        )\n",
    "\n",
    "        masks = np.zeros((self.image_size[1], self.image_size[0], len(self.TRAFFIC_LIGHT_CLASSES)), dtype=np.float32)\n",
    "\n",
    "        for light in traffic_lights:\n",
    "            class_idx = self.class_to_index[light[\"class\"]]\n",
    "            scaled_bbox = self.scale_bbox(light[\"bbox\"])\n",
    "\n",
    "            # 座標を適切な範囲に収める\n",
    "            x1 = self._convert_coordinate(scaled_bbox[0], self.image_size[0])\n",
    "            y1 = self._convert_coordinate(scaled_bbox[1], self.image_size[1])\n",
    "            x2 = self._convert_coordinate(scaled_bbox[2], self.image_size[0])\n",
    "            y2 = self._convert_coordinate(scaled_bbox[3], self.image_size[1])\n",
    "\n",
    "            masks[y1 : y2 + 1, x1 : x2 + 1, class_idx] = 1.0\n",
    "\n",
    "        return masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"vEgo\",\n",
    "    \"aEgo\",\n",
    "    \"steeringAngleDeg\",\n",
    "    \"steeringTorque\",\n",
    "    \"gas\",\n",
    "    \"leftBlinker\",\n",
    "    \"rightBlinker\",\n",
    "    \"brakePressed\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, json_dir, depth_dir, transform=None, is_train=True):\n",
    "        self.df = df\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.json_dir = Path(json_dir)\n",
    "        self.depth_dir = Path(depth_dir)\n",
    "        self.is_train = is_train\n",
    "        self.feature_cols = [\n",
    "            \"vEgo\",\n",
    "            \"aEgo\",\n",
    "            \"steeringAngleDeg\",\n",
    "            \"steeringTorque\",\n",
    "            \"gas\",\n",
    "            \"leftBlinker\",\n",
    "            \"rightBlinker\",\n",
    "            \"brakePressed\",\n",
    "        ]\n",
    "\n",
    "        # オリジナル画像用の変換処理\n",
    "        if transform is None:\n",
    "            self.transform = A.ReplayCompose(\n",
    "                [\n",
    "                    A.Resize(cfg.cnn.size, cfg.cnn.size),\n",
    "                    A.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406],  # 通常のImageNet平均値\n",
    "                        std=[0.229, 0.224, 0.225],  # 通常のImageNet標準偏差\n",
    "                    ),\n",
    "                    ToTensorV2(),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        # Depth Map用の変換処理\n",
    "        self.depth_transform = A.ReplayCompose(\n",
    "            [\n",
    "                A.Resize(cfg.cnn.size, cfg.cnn.size),\n",
    "                A.Normalize(mean=[0.5], std=[0.5]),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.target_cols = cfg.target_cols\n",
    "        self.mask_generator = TrafficLightMaskGenerator(cfg.cnn.size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df[idx]\n",
    "        img_folder = self.img_dir / row[\"ID\"].item()\n",
    "        depth_folder = self.depth_dir / row[\"ID\"].item()\n",
    "        json_path = self.json_dir / f\"{row['ID'].item()}.json\"\n",
    "\n",
    "        # 3枚の画像を読み込み\n",
    "        img_names = [\"image_t-1.0.png\", \"image_t-0.5.png\", \"image_t.png\"]\n",
    "        imgs = []\n",
    "        depths = []\n",
    "\n",
    "        for img_name in img_names:\n",
    "            # オリジナル画像の読み込み\n",
    "            img_path = img_folder / img_name\n",
    "            img = cv2.imread(str(img_path))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            imgs.append(img)\n",
    "\n",
    "            # Depth Mapの読み込み\n",
    "            depth_path = depth_folder / img_name\n",
    "            depth = cv2.imread(str(depth_path), cv2.IMREAD_GRAYSCALE)\n",
    "            depths.append(depth)\n",
    "\n",
    "        # オリジナル画像の変換\n",
    "        if self.transform:\n",
    "            replay = None\n",
    "            transformed_imgs = []\n",
    "\n",
    "            for img in imgs:\n",
    "                if replay is None:\n",
    "                    # 最初の画像に対して変換を実行し、replayを保存\n",
    "                    transformed = self.transform(image=img)\n",
    "                    replay = transformed[\"replay\"]\n",
    "                else:\n",
    "                    # 2枚目以降は保存したreplayを使用\n",
    "                    transformed = A.ReplayCompose.replay(replay, image=img)\n",
    "                transformed_imgs.append(transformed[\"image\"])\n",
    "\n",
    "        # Depth Mapの変換\n",
    "        transformed_depths = []\n",
    "        for depth in depths:\n",
    "            transformed_depth = self.depth_transform(image=depth)\n",
    "            transformed_depths.append(transformed_depth[\"image\"])\n",
    "\n",
    "        # オリジナル画像をチャネル方向に結合 (C*3, H, W)\n",
    "        img_tensor = torch.cat(transformed_imgs, dim=0)\n",
    "\n",
    "        # Depth Mapをチャネル方向に結合 (3, H, W)\n",
    "        depth_tensor = torch.cat(transformed_depths, dim=0)\n",
    "\n",
    "        # 信号機マスクの生成\n",
    "        mask = self.mask_generator.generate_masks(json_path)\n",
    "        mask_tensor = torch.from_numpy(mask).permute(2, 0, 1)  # (8, H, W)\n",
    "\n",
    "        # 画像, Depth Map, 信号機マスクをチャネル方向に結合 (C*3+3+8=20, H, W)\n",
    "        combined_tensor = torch.cat([img_tensor, depth_tensor, mask_tensor], dim=0)\n",
    "\n",
    "        # 数値特徴量の取得\n",
    "        feature_tensor = torch.tensor(row[feature_cols].to_numpy(), dtype=torch.float32).squeeze(0)\n",
    "\n",
    "        # ターゲットの準備\n",
    "        if self.is_train:\n",
    "            target = torch.tensor(row[self.target_cols].to_numpy(), dtype=torch.float32).squeeze(0)\n",
    "            return combined_tensor, feature_tensor, target\n",
    "        else:\n",
    "            return combined_tensor, feature_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6, p_trainable=True):\n",
    "        super().__init__()\n",
    "        if p_trainable:\n",
    "            self.p = nn.Parameter(torch.ones(1) * p)\n",
    "        else:\n",
    "            self.p = p\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.clamp(min=self.eps).pow(self.p)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = x.pow(1.0 / self.p)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, pretrained=False, target_size=None, model_name=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=cfg.pretrained,\n",
    "            num_classes=0,\n",
    "            in_chans=cfg.in_chans,\n",
    "            global_pool=\"\",\n",
    "        )\n",
    "\n",
    "        self.n_features = self.encoder.num_features\n",
    "        self.pool = GeM(p=3, eps=1e-6, p_trainable=True)\n",
    "        self.n_numeric_features = 8  # 数値特徴量の次元数\n",
    "\n",
    "        self.target_size = cfg.target_size if target_size is None else target_size\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.n_features + self.n_numeric_features, 512),  # 画像特徴量と数値特徴量を結合\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.target_size if target_size is None else target_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, image, numeric_features):\n",
    "        feature = self.encoder(image)\n",
    "        feature = self.pool(feature).reshape(feature.size(0), -1)\n",
    "        combined_features = torch.cat([feature, numeric_features], dim=1)\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModel(cfg.cnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "if DEBUG:\n",
    "    cfg.cnn.num_epochs = 1\n",
    "    train = train.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created: ../../results/009/20241124_232225\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s][ WARN:0@3081.763] global loadsave.cpp:241 findDecoder imread_('../../data/input/depth/000fb056f97572d384bae4f5fc1e0f28_20/image_t-1.0.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@3081.764] global loadsave.cpp:241 findDecoder imread_('../../data/input/depth/000fb056f97572d384bae4f5fc1e0f28_20/image_t-0.5.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@3081.764] global loadsave.cpp:241 findDecoder imread_('../../data/input/depth/000fb056f97572d384bae4f5fc1e0f28_20/image_t.png'): can't open/read file: check file path/integrity\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "image must be numpy array type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mcnn\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[1;32m     40\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnumeric_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/compe/atmacup_18/.venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/Documents/compe/atmacup_18/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/compe/atmacup_18/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/compe/atmacup_18/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[71], line 90\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     88\u001b[0m transformed_depths \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m depth \u001b[38;5;129;01min\u001b[39;00m depths:\n\u001b[0;32m---> 90\u001b[0m     transformed_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdepth_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     transformed_depths\u001b[38;5;241m.\u001b[39mappend(transformed_depth[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# オリジナル画像をチャネル方向に結合 (C*3, H, W)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/compe/atmacup_18/.venv/lib/python3.12/site-packages/albumentations/core/composition.py:804\u001b[0m, in \u001b[0;36mReplayCompose.__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, force_apply: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    803\u001b[0m     kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_key] \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m--> 804\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mforce_apply\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_apply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m     serialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dict_with_id()\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_with_params(serialized, result[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_key])\n",
      "File \u001b[0;32m~/Documents/compe/atmacup_18/.venv/lib/python3.12/site-packages/albumentations/core/composition.py:421\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m need_to_run:\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m    424\u001b[0m     data \u001b[38;5;241m=\u001b[39m t(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n",
      "File \u001b[0;32m~/Documents/compe/atmacup_18/.venv/lib/python3.12/site-packages/albumentations/core/composition.py:442\u001b[0m, in \u001b[0;36mCompose.preprocess\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_check_args:\n\u001b[0;32m--> 442\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain_compose:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m~/Documents/compe/atmacup_18/.venv/lib/python3.12/site-packages/albumentations/core/composition.py:534\u001b[0m, in \u001b[0;36mCompose._check_args\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m internal_data_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_additional_targets\u001b[38;5;241m.\u001b[39mget(data_name, data_name)\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m internal_data_name \u001b[38;5;129;01min\u001b[39;00m CHECKED_SINGLE:\n\u001b[0;32m--> 534\u001b[0m     shapes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_single_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m internal_data_name \u001b[38;5;129;01min\u001b[39;00m CHECKED_MULTI \u001b[38;5;129;01mand\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m internal_data_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/compe/atmacup_18/.venv/lib/python3.12/site-packages/albumentations/core/composition.py:487\u001b[0m, in \u001b[0;36mCompose._check_single_data\u001b[0;34m(data_name, data)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_single_data\u001b[39m(data_name: \u001b[38;5;28mstr\u001b[39m, data: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 487\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be numpy array type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: image must be numpy array type"
     ]
    }
   ],
   "source": [
    "# 実験結果格納用のディレクトリを作成\n",
    "japan_tz = pytz.timezone(\"Asia/Tokyo\")\n",
    "cfg.run_time = datetime.now(japan_tz).strftime(\"%Y%m%d_%H%M%S\")\n",
    "create_dir(cfg.data.results_path)\n",
    "\n",
    "# CV用の配列を初期化\n",
    "oof_predictions = np.zeros((len(train), len(cfg.target_cols)))\n",
    "models = {}\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf.split(train, groups=train[\"scene\"])):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    # データセットの作成\n",
    "    train_dataset = CustomDataset(\n",
    "        train[train_idx], cfg.data.img_root, cfg.data.json_root, cfg.data.depth_root, transform=get_train_transform()\n",
    "    )\n",
    "    valid_dataset = CustomDataset(\n",
    "        train[valid_idx], cfg.data.img_root, cfg.data.json_root, cfg.data.depth_root, transform=get_valid_transform()\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.cnn.batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=cfg.cnn.batch_size, shuffle=False)\n",
    "\n",
    "    # モデル、損失関数、オプティマイザーの初期化\n",
    "    model = CustomModel(cfg.cnn).to(device)\n",
    "    criterion = nn.HuberLoss()\n",
    "    # criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.cnn.lr)\n",
    "    total_steps = len(train_loader) * cfg.cnn.num_epochs\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=total_steps * 0.1,\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    # 学習ループ\n",
    "    for epoch in range(cfg.cnn.num_epochs):\n",
    "        model.train()\n",
    "        for images, numeric_features, targets in tqdm(train_loader):\n",
    "            images = images.to(device)\n",
    "            numeric_features = numeric_features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, numeric_features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # 検証\n",
    "        model.eval()\n",
    "        valid_losses = []\n",
    "        with torch.no_grad():\n",
    "            for images, numeric_features, targets in valid_loader:\n",
    "                images = images.to(device)\n",
    "                numeric_features = numeric_features.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(images, numeric_features)\n",
    "                loss = criterion(outputs, targets)\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        print(f\"Epoch {epoch + 1}, Valid Loss: {valid_loss:.4f}\")\n",
    "\n",
    "        # ベストモデルの保存\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f\"{cfg.data.results_path}/model_fold{fold}.pth\")\n",
    "\n",
    "    # ベストモデルでOOF予測\n",
    "    model.load_state_dict(torch.load(f\"{cfg.data.results_path}/model_fold{fold}.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    valid_dataset = CustomDataset(\n",
    "        train[valid_idx], cfg.data.img_root, cfg.data.json_root, cfg.data.depth_root, transform=get_valid_transform()\n",
    "    )\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=cfg.cnn.batch_size, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, numeric_features) in enumerate(valid_loader):\n",
    "            images = images.to(device)\n",
    "            numeric_features = numeric_features.to(device)\n",
    "            outputs = model(images, numeric_features)\n",
    "            start_idx = i * cfg.cnn.batch_size\n",
    "            end_idx = start_idx + outputs.shape[0]\n",
    "            oof_predictions[valid_idx[start_idx:end_idx]] = outputs.cpu().numpy()\n",
    "\n",
    "# CVスコアの計算（MAEの平均）\n",
    "mae_scores = []\n",
    "for i in range(len(cfg.target_cols)):\n",
    "    mae = np.mean(np.abs(oof_predictions[:, i] - train[cfg.target_cols[i]].to_numpy()))\n",
    "    mae_scores.append(mae)\n",
    "\n",
    "cv_score = np.mean(mae_scores)\n",
    "print(f\"CV Score: {cv_score:.4f}\")\n",
    "\n",
    "# oofを保存\n",
    "np.save(f\"{cfg.data.results_path}/oof_predictions.npy\", oof_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using fold 1 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1568158/236869104.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"{cfg.data.results_path}/model_fold{fold}.pth\"))\n",
      "100%|██████████| 54/54 [00:13<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using fold 2 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:09<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using fold 3 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:10<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting using fold 4 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:10<00:00,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 18)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>x_0</th><th>y_0</th><th>z_0</th><th>x_1</th><th>y_1</th><th>z_1</th><th>x_2</th><th>y_2</th><th>z_2</th><th>x_3</th><th>y_3</th><th>z_3</th><th>x_4</th><th>y_4</th><th>z_4</th><th>x_5</th><th>y_5</th><th>z_5</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1.910416</td><td>0.015323</td><td>-0.00203</td><td>4.111138</td><td>0.049189</td><td>0.001671</td><td>6.400317</td><td>0.093142</td><td>0.001914</td><td>8.764425</td><td>0.14443</td><td>0.008326</td><td>11.181863</td><td>0.203873</td><td>0.013329</td><td>13.635955</td><td>0.27344</td><td>0.019102</td></tr><tr><td>0.888904</td><td>0.273903</td><td>-0.002772</td><td>1.869887</td><td>0.726532</td><td>0.002171</td><td>2.899538</td><td>1.31719</td><td>0.011759</td><td>3.96669</td><td>2.022292</td><td>0.024279</td><td>5.108733</td><td>2.831406</td><td>0.041865</td><td>6.321622</td><td>3.753189</td><td>0.058737</td></tr><tr><td>2.079381</td><td>0.006609</td><td>-0.00427</td><td>4.220596</td><td>0.019128</td><td>-0.009004</td><td>6.19882</td><td>0.029642</td><td>-0.013304</td><td>8.010659</td><td>0.044004</td><td>-0.018825</td><td>9.672832</td><td>0.050245</td><td>-0.025318</td><td>11.221782</td><td>0.051523</td><td>-0.033896</td></tr><tr><td>0.916636</td><td>0.02805</td><td>-0.007229</td><td>1.805815</td><td>0.074985</td><td>-0.013208</td><td>2.588893</td><td>0.138594</td><td>-0.01687</td><td>3.264247</td><td>0.217395</td><td>-0.021215</td><td>3.85386</td><td>0.317263</td><td>-0.025356</td><td>4.371171</td><td>0.43165</td><td>-0.027858</td></tr><tr><td>1.283522</td><td>-0.002682</td><td>-0.005985</td><td>2.565668</td><td>-0.007541</td><td>-0.012687</td><td>3.715343</td><td>-0.017339</td><td>-0.020631</td><td>4.736634</td><td>-0.026279</td><td>-0.029341</td><td>5.637933</td><td>-0.035247</td><td>-0.039212</td><td>6.447492</td><td>-0.048434</td><td>-0.048612</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 18)\n",
       "┌──────────┬───────────┬───────────┬──────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ x_0      ┆ y_0       ┆ z_0       ┆ x_1      ┆ … ┆ z_4       ┆ x_5       ┆ y_5       ┆ z_5       │\n",
       "│ ---      ┆ ---       ┆ ---       ┆ ---      ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│ f64      ┆ f64       ┆ f64       ┆ f64      ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64       │\n",
       "╞══════════╪═══════════╪═══════════╪══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 1.910416 ┆ 0.015323  ┆ -0.00203  ┆ 4.111138 ┆ … ┆ 0.013329  ┆ 13.635955 ┆ 0.27344   ┆ 0.019102  │\n",
       "│ 0.888904 ┆ 0.273903  ┆ -0.002772 ┆ 1.869887 ┆ … ┆ 0.041865  ┆ 6.321622  ┆ 3.753189  ┆ 0.058737  │\n",
       "│ 2.079381 ┆ 0.006609  ┆ -0.00427  ┆ 4.220596 ┆ … ┆ -0.025318 ┆ 11.221782 ┆ 0.051523  ┆ -0.033896 │\n",
       "│ 0.916636 ┆ 0.02805   ┆ -0.007229 ┆ 1.805815 ┆ … ┆ -0.025356 ┆ 4.371171  ┆ 0.43165   ┆ -0.027858 │\n",
       "│ 1.283522 ┆ -0.002682 ┆ -0.005985 ┆ 2.565668 ┆ … ┆ -0.039212 ┆ 6.447492  ┆ -0.048434 ┆ -0.048612 │\n",
       "└──────────┴───────────┴───────────┴──────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testの推論\n",
    "test_dataset = CustomDataset(\n",
    "    test, cfg.data.img_root, cfg.data.json_root, cfg.data.depth_root, transform=get_valid_transform(), is_train=False\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.cnn.batch_size, shuffle=False)\n",
    "\n",
    "# fold分の予測値を格納する配列\n",
    "test_predictions = np.zeros((len(test), len(cfg.target_cols), cfg.n_splits))\n",
    "\n",
    "# 各foldのモデルで予測\n",
    "for fold in range(cfg.n_splits):\n",
    "    print(f\"Predicting using fold {fold + 1} model\")\n",
    "\n",
    "    # モデルの読み込み\n",
    "    model = CustomModel(cfg.cnn).to(device)\n",
    "    model.load_state_dict(torch.load(f\"{cfg.data.results_path}/model_fold{fold}.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    fold_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            fold_predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "    # バッチごとの予測を結合\n",
    "    fold_predictions = np.concatenate(fold_predictions, axis=0)\n",
    "    test_predictions[:, :, fold] = fold_predictions\n",
    "\n",
    "# fold分の予測値の平均を計算\n",
    "final_predictions = test_predictions.mean(axis=2)\n",
    "\n",
    "# submissionファイルの作成\n",
    "exprs = [pl.Series(final_predictions[:, i]).alias(cfg.target_cols[i]) for i in range(len(cfg.target_cols))]\n",
    "submission = sample_submission.with_columns(exprs)\n",
    "submission.write_csv(f\"{cfg.data.results_path}/submission.csv\")\n",
    "print(\"Submission file created!\")\n",
    "\n",
    "# 確認\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_predictionsを保存\n",
    "np.save(f\"{cfg.data.results_path}/final_predictions.npy\", final_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1727, 18)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
