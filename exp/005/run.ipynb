{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marumarukun/pj/compe/atma_18/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_number: '005'\n",
      "run_time: base\n",
      "data:\n",
      "  input_root: ../../data/input\n",
      "  train_path: ../../data/input/train_features.csv\n",
      "  test_path: ../../data/input/test_features.csv\n",
      "  sample_submission_path: ../../data/input/sample_submission.csv\n",
      "  img_root: ../../data/input/images\n",
      "  output_root: ../../data/output\n",
      "  results_root: ../../results\n",
      "  results_path: ../../results/005/base\n",
      "seed: 319\n",
      "n_splits: 5\n",
      "target_cols:\n",
      "- x_0\n",
      "- y_0\n",
      "- z_0\n",
      "- x_1\n",
      "- y_1\n",
      "- z_1\n",
      "- x_2\n",
      "- y_2\n",
      "- z_2\n",
      "- x_3\n",
      "- y_3\n",
      "- z_3\n",
      "- x_4\n",
      "- y_4\n",
      "- z_4\n",
      "- x_5\n",
      "- y_5\n",
      "- z_5\n",
      "cnn:\n",
      "  model_name: tf_efficientnet_b0_ns\n",
      "  size: 224\n",
      "  pretrained: true\n",
      "  in_chans: 3\n",
      "  target_size: 18\n",
      "  lr: 0.0001\n",
      "  num_epochs: 20\n",
      "  batch_size: 32\n",
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pprint\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pytz\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from albumentations import ReplayCompose\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from src.config import cfg\n",
    "from src.dir import create_dir\n",
    "from src.seed import seed_everything\n",
    "\n",
    "cfg.exp_number = Path().resolve().name\n",
    "print(OmegaConf.to_yaml(cfg, resolve=True))\n",
    "\n",
    "seed_everything(cfg.seed)\n",
    "pl.Config.set_fmt_str_lengths(1000)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exp005\n",
    "\n",
    "- ちょっと凝ったNNモデルを作ってみるnotebook\n",
    "- アーキテクチャ\n",
    "    - エンコーダー: EfficientNet-B0\n",
    "    - プーリング層: GeM\n",
    "    - Transformerレイヤー: 1層\n",
    "    - 位置エンコーディング: 1, 3, n_features\n",
    "    - 全結合層: 2層\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>fold</th><th>count</th></tr><tr><td>i32</td><td>u32</td></tr></thead><tbody><tr><td>5</td><td>8674</td></tr><tr><td>3</td><td>8674</td></tr><tr><td>2</td><td>8674</td></tr><tr><td>4</td><td>8674</td></tr><tr><td>1</td><td>8675</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌──────┬───────┐\n",
       "│ fold ┆ count │\n",
       "│ ---  ┆ ---   │\n",
       "│ i32  ┆ u32   │\n",
       "╞══════╪═══════╡\n",
       "│ 5    ┆ 8674  │\n",
       "│ 3    ┆ 8674  │\n",
       "│ 2    ┆ 8674  │\n",
       "│ 4    ┆ 8674  │\n",
       "│ 1    ┆ 8675  │\n",
       "└──────┴───────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データの読み込み\n",
    "train = pl.read_csv(cfg.data.train_path, try_parse_dates=True)\n",
    "test = pl.read_csv(cfg.data.test_path, try_parse_dates=True)\n",
    "sample_submission = pl.read_csv(cfg.data.sample_submission_path, try_parse_dates=True)\n",
    "\n",
    "# index列を作成\n",
    "train = train.with_row_index()\n",
    "test = test.with_row_index()\n",
    "\n",
    "# データの結合(label encoding用)\n",
    "train_test = pl.concat([train, test], how=\"diagonal\")\n",
    "\n",
    "# scene列を作成 → これでGroupKFoldする\n",
    "train = train.with_columns(pl.col(\"ID\").str.split(\"_\").list[0].alias(\"scene\"))\n",
    "test = test.with_columns(pl.col(\"ID\").str.split(\"_\").list[0].alias(\"scene\"))\n",
    "\n",
    "# CV\n",
    "gkf = GroupKFold(n_splits=cfg.n_splits)\n",
    "\n",
    "# fold列を作成\n",
    "train = train.with_columns(pl.lit(-1).alias(\"fold\"))\n",
    "for i, (_, valid_idx) in enumerate(gkf.split(train, groups=train[\"scene\"])):\n",
    "    fold = i + 1\n",
    "    train = train.with_columns(\n",
    "        pl.when(pl.col(\"index\").is_in(valid_idx)).then(fold).otherwise(pl.col(\"fold\")).alias(\"fold\")\n",
    "    )\n",
    "\n",
    "train[\"fold\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 33)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>index</th><th>ID</th><th>vEgo</th><th>aEgo</th><th>steeringAngleDeg</th><th>steeringTorque</th><th>brake</th><th>brakePressed</th><th>gas</th><th>gasPressed</th><th>gearShifter</th><th>leftBlinker</th><th>rightBlinker</th><th>x_0</th><th>y_0</th><th>z_0</th><th>x_1</th><th>y_1</th><th>z_1</th><th>x_2</th><th>y_2</th><th>z_2</th><th>x_3</th><th>y_3</th><th>z_3</th><th>x_4</th><th>y_4</th><th>z_4</th><th>x_5</th><th>y_5</th><th>z_5</th><th>scene</th><th>fold</th></tr><tr><td>u32</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>bool</td><td>f64</td><td>bool</td><td>str</td><td>bool</td><td>bool</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>i32</td></tr></thead><tbody><tr><td>0</td><td>&quot;00066be8e20318869c38c66be466631a_320&quot;</td><td>5.701526</td><td>1.538456</td><td>-2.165777</td><td>-139.0</td><td>0.0</td><td>false</td><td>0.25</td><td>true</td><td>&quot;drive&quot;</td><td>false</td><td>false</td><td>2.82959</td><td>0.032226</td><td>0.045187</td><td>6.231999</td><td>0.065895</td><td>0.107974</td><td>9.785009</td><td>0.124972</td><td>0.203649</td><td>13.485472</td><td>0.163448</td><td>0.302818</td><td>17.574227</td><td>0.174289</td><td>0.406331</td><td>21.951269</td><td>0.199503</td><td>0.485079</td><td>&quot;00066be8e20318869c38c66be466631a&quot;</td><td>1</td></tr><tr><td>1</td><td>&quot;00066be8e20318869c38c66be466631a_420&quot;</td><td>11.176292</td><td>0.279881</td><td>-11.625697</td><td>-44.0</td><td>0.0</td><td>false</td><td>0.0</td><td>false</td><td>&quot;drive&quot;</td><td>false</td><td>true</td><td>4.970268</td><td>-0.007936</td><td>0.005028</td><td>10.350489</td><td>-0.032374</td><td>-0.020701</td><td>15.770054</td><td>0.084073</td><td>0.008645</td><td>21.132415</td><td>0.391343</td><td>0.036335</td><td>26.316489</td><td>0.843124</td><td>0.065</td><td>31.383814</td><td>1.42507</td><td>0.073083</td><td>&quot;00066be8e20318869c38c66be466631a&quot;</td><td>1</td></tr><tr><td>2</td><td>&quot;00066be8e20318869c38c66be466631a_520&quot;</td><td>10.472548</td><td>0.231099</td><td>-2.985105</td><td>-132.0</td><td>0.0</td><td>false</td><td>0.18</td><td>true</td><td>&quot;drive&quot;</td><td>false</td><td>false</td><td>4.815701</td><td>-0.000813</td><td>0.017577</td><td>10.153522</td><td>-0.0278</td><td>0.026165</td><td>15.446539</td><td>-0.155987</td><td>0.040397</td><td>20.61816</td><td>-0.356932</td><td>0.058765</td><td>25.677387</td><td>-0.576985</td><td>0.102859</td><td>30.460033</td><td>-0.841894</td><td>0.152889</td><td>&quot;00066be8e20318869c38c66be466631a&quot;</td><td>1</td></tr><tr><td>3</td><td>&quot;000fb056f97572d384bae4f5fc1e0f28_120&quot;</td><td>6.055565</td><td>-0.117775</td><td>7.632668</td><td>173.0</td><td>0.0</td><td>false</td><td>0.0</td><td>false</td><td>&quot;drive&quot;</td><td>false</td><td>false</td><td>2.812608</td><td>0.033731</td><td>0.0059</td><td>5.975378</td><td>0.137848</td><td>0.01621</td><td>9.186793</td><td>0.322997</td><td>0.031626</td><td>12.37311</td><td>0.603145</td><td>0.031858</td><td>15.703514</td><td>0.960717</td><td>0.043479</td><td>19.311182</td><td>1.374655</td><td>0.058754</td><td>&quot;000fb056f97572d384bae4f5fc1e0f28&quot;</td><td>3</td></tr><tr><td>4</td><td>&quot;000fb056f97572d384bae4f5fc1e0f28_20&quot;</td><td>3.316744</td><td>1.276733</td><td>-31.725477</td><td>-114.0</td><td>0.0</td><td>false</td><td>0.255</td><td>true</td><td>&quot;drive&quot;</td><td>false</td><td>false</td><td>1.55186</td><td>-0.041849</td><td>-0.008847</td><td>3.675162</td><td>-0.125189</td><td>-0.013725</td><td>6.113567</td><td>-0.239161</td><td>-0.012887</td><td>8.770783</td><td>-0.381813</td><td>-0.003898</td><td>11.619313</td><td>-0.554488</td><td>0.011393</td><td>14.657048</td><td>-0.7788</td><td>0.044243</td><td>&quot;000fb056f97572d384bae4f5fc1e0f28&quot;</td><td>3</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 33)\n",
       "┌───────┬────────────────┬───────────┬───────────┬───┬───────────┬──────────┬───────────────┬──────┐\n",
       "│ index ┆ ID             ┆ vEgo      ┆ aEgo      ┆ … ┆ y_5       ┆ z_5      ┆ scene         ┆ fold │\n",
       "│ ---   ┆ ---            ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---      ┆ ---           ┆ ---  │\n",
       "│ u32   ┆ str            ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64      ┆ str           ┆ i32  │\n",
       "╞═══════╪════════════════╪═══════════╪═══════════╪═══╪═══════════╪══════════╪═══════════════╪══════╡\n",
       "│ 0     ┆ 00066be8e20318 ┆ 5.701526  ┆ 1.538456  ┆ … ┆ 0.199503  ┆ 0.485079 ┆ 00066be8e2031 ┆ 1    │\n",
       "│       ┆ 869c38c66be466 ┆           ┆           ┆   ┆           ┆          ┆ 8869c38c66be4 ┆      │\n",
       "│       ┆ 631a_320       ┆           ┆           ┆   ┆           ┆          ┆ 66631a        ┆      │\n",
       "│ 1     ┆ 00066be8e20318 ┆ 11.176292 ┆ 0.279881  ┆ … ┆ 1.42507   ┆ 0.073083 ┆ 00066be8e2031 ┆ 1    │\n",
       "│       ┆ 869c38c66be466 ┆           ┆           ┆   ┆           ┆          ┆ 8869c38c66be4 ┆      │\n",
       "│       ┆ 631a_420       ┆           ┆           ┆   ┆           ┆          ┆ 66631a        ┆      │\n",
       "│ 2     ┆ 00066be8e20318 ┆ 10.472548 ┆ 0.231099  ┆ … ┆ -0.841894 ┆ 0.152889 ┆ 00066be8e2031 ┆ 1    │\n",
       "│       ┆ 869c38c66be466 ┆           ┆           ┆   ┆           ┆          ┆ 8869c38c66be4 ┆      │\n",
       "│       ┆ 631a_520       ┆           ┆           ┆   ┆           ┆          ┆ 66631a        ┆      │\n",
       "│ 3     ┆ 000fb056f97572 ┆ 6.055565  ┆ -0.117775 ┆ … ┆ 1.374655  ┆ 0.058754 ┆ 000fb056f9757 ┆ 3    │\n",
       "│       ┆ d384bae4f5fc1e ┆           ┆           ┆   ┆           ┆          ┆ 2d384bae4f5fc ┆      │\n",
       "│       ┆ 0f28_120       ┆           ┆           ┆   ┆           ┆          ┆ 1e0f28        ┆      │\n",
       "│ 4     ┆ 000fb056f97572 ┆ 3.316744  ┆ 1.276733  ┆ … ┆ -0.7788   ┆ 0.044243 ┆ 000fb056f9757 ┆ 3    │\n",
       "│       ┆ d384bae4f5fc1e ┆           ┆           ┆   ┆           ┆          ┆ 2d384bae4f5fc ┆      │\n",
       "│       ┆ 0f28_20        ┆           ┆           ┆   ┆           ┆          ┆ 1e0f28        ┆      │\n",
       "└───────┴────────────────┴───────────┴───────────┴───┴───────────┴──────────┴───────────────┴──────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ拡張\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transform():\n",
    "    return A.ReplayCompose(\n",
    "        [\n",
    "            # 1. 基本的なサイズ調整（必須）\n",
    "            A.Resize(cfg.cnn.size, cfg.cnn.size),\n",
    "            # 2. 軽量な明るさ調整（高速で効果が高い）\n",
    "            A.ColorJitter(\n",
    "                brightness=0.2,\n",
    "                contrast=0.2,\n",
    "                saturation=0.1,\n",
    "                hue=0.0,  # 色相の変更は省略\n",
    "                p=0.7,\n",
    "            ),\n",
    "            # # 3. 軽量なブレ表現（1つだけ選択）\n",
    "            # A.OneOf(\n",
    "            #     [\n",
    "            #         A.MotionBlur(blur_limit=3),\n",
    "            #         A.GaussianBlur(blur_limit=3),\n",
    "            #     ],\n",
    "            #     p=0.3,\n",
    "            # ),\n",
    "            # # 4. 軽量なノイズ（シンプルな設定）\n",
    "            # A.GaussNoise(\n",
    "            #     var_limit=(10, 30),\n",
    "            #     mean=0,\n",
    "            #     per_channel=False,  # チャンネル共通で処理\n",
    "            #     p=0.3,\n",
    "            # ),\n",
    "            # 5. 正規化（必須）\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transform():\n",
    "    return A.ReplayCompose(\n",
    "        [\n",
    "            A.Resize(cfg.cnn.size, cfg.cnn.size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None, is_train=True):\n",
    "        self.df = df\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.is_train = is_train\n",
    "\n",
    "        # デフォルトの変換処理\n",
    "        if transform is None:\n",
    "            self.transform = A.ReplayCompose(\n",
    "                [\n",
    "                    A.Resize(cfg.cnn.size, cfg.cnn.size),\n",
    "                    A.Normalize(\n",
    "                        mean=[0.485, 0.456, 0.406],  # 通常のImageNet平均値\n",
    "                        std=[0.229, 0.224, 0.225],  # 通常のImageNet標準偏差\n",
    "                    ),\n",
    "                    ToTensorV2(),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        self.target_cols = cfg.target_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df[idx]\n",
    "        img_folder = self.img_dir / row[\"ID\"].item()\n",
    "\n",
    "        # 3枚の画像を読み込み\n",
    "        img_names = [\"image_t-1.0.png\", \"image_t-0.5.png\", \"image_t.png\"]\n",
    "        imgs = []\n",
    "\n",
    "        for img_name in img_names:\n",
    "            img_path = img_folder / img_name\n",
    "            img = cv2.imread(str(img_path))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            imgs.append(img)\n",
    "\n",
    "        if self.transform:\n",
    "            replay = None\n",
    "            transformed_imgs = []\n",
    "\n",
    "            for img in imgs:\n",
    "                if replay is None:\n",
    "                    # 最初の画像に対して変換を実行し、replayを保存\n",
    "                    transformed = self.transform(image=img)\n",
    "                    replay = transformed[\"replay\"]\n",
    "                else:\n",
    "                    # 2枚目以降は保存したreplayを使用\n",
    "                    transformed = A.ReplayCompose.replay(replay, image=img)\n",
    "                transformed_imgs.append(transformed[\"image\"])\n",
    "\n",
    "        # チャネル方向に結合 (C*3, H, W)\n",
    "        img_tensor = torch.cat(transformed_imgs, dim=0)\n",
    "\n",
    "        # ターゲットの準備\n",
    "        if self.is_train:\n",
    "            target = torch.tensor(row[self.target_cols].to_numpy(), dtype=torch.float32).squeeze(0)\n",
    "            return img_tensor, target\n",
    "        else:\n",
    "            return img_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6, p_trainable=True):\n",
    "        super().__init__()\n",
    "        if p_trainable:\n",
    "            self.p = nn.Parameter(torch.ones(1) * p)\n",
    "        else:\n",
    "            self.p = p\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.clamp(min=self.eps).pow(self.p)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = x.pow(1.0 / self.p)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, pretrained=False, target_size=None, model_name=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # エンコーダーとプーリング層\n",
    "        self.encoder = timm.create_model(\n",
    "            cfg.model_name, pretrained=cfg.pretrained, num_classes=0, in_chans=cfg.in_chans\n",
    "        )\n",
    "        self.n_features = self.encoder.num_features  # 例：1280 (EfficientNet-B0の場合)\n",
    "        # self.pool = GeM(p=3, eps=1e-6, p_trainable=True)\n",
    "\n",
    "        # Transformerレイヤー\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.n_features,  # 特徴量の次元数\n",
    "            nhead=8,  # アテンションヘッド数\n",
    "            dim_feedforward=self.n_features * 4,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "\n",
    "        # 位置エンコーディング: (1, 3, n_features)\n",
    "        # 3は時系列の長さ（3つの時点）\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 3, self.n_features))\n",
    "\n",
    "        self.target_size = cfg.target_size if target_size is None else target_size\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.n_features * 3, self.n_features * 2),\n",
    "            nn.LayerNorm(self.n_features * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.n_features * 2, self.target_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 入力 x: (batch_size, 9, H, W)\n",
    "        # 9 = 3チャネル × 3時点\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # 3フレームに分割\n",
    "        # 各frame: (batch_size, 3, H, W)\n",
    "        frames = x.chunk(3, dim=1)\n",
    "\n",
    "        # 各フレームから特徴抽出\n",
    "        features = []\n",
    "        for frame in frames:\n",
    "            # encoder出力: (batch_size, n_features, h, w)\n",
    "            feat = self.encoder(frame)\n",
    "            # pool後: (batch_size, n_features)\n",
    "            # feat = self.pool(feat).reshape(batch_size, -1)\n",
    "            features.append(feat)\n",
    "\n",
    "        # 時系列データとしてスタック\n",
    "        # sequence: (batch_size, 3, n_features)\n",
    "        sequence = torch.stack(features, dim=1)\n",
    "\n",
    "        # 位置エンコーディングを追加\n",
    "        # sequence: (batch_size, 3, n_features)\n",
    "        sequence = sequence + self.pos_embedding\n",
    "\n",
    "        # Transformer処理\n",
    "        # sequence: (batch_size, 3, n_features)\n",
    "        sequence = self.transformer(sequence)\n",
    "\n",
    "        # 全結合層\n",
    "        # reshape: (batch_size, 3 * n_features)\n",
    "        # 最終出力: (batch_size, target_size)\n",
    "        out = self.fc(sequence.reshape(batch_size, -1))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    cfg.cnn.num_epochs = 1\n",
    "    train = train.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created: ../../results/005/20241121_232428\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marumarukun/pj/compe/atma_18/.venv/lib/python3.12/site-packages/pydantic/main.py:212: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "/home/marumarukun/pj/compe/atma_18/.venv/lib/python3.12/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name tf_efficientnet_b0_ns to current tf_efficientnet_b0.ns_jft_in1k.\n",
      "  model = create_fn(\n",
      " 15%|█▌        | 168/1085 [01:19<06:59,  2.19it/s]"
     ]
    }
   ],
   "source": [
    "# 実験結果格納用のディレクトリを作成\n",
    "japan_tz = pytz.timezone(\"Asia/Tokyo\")\n",
    "cfg.run_time = datetime.now(japan_tz).strftime(\"%Y%m%d_%H%M%S\")\n",
    "create_dir(cfg.data.results_path)\n",
    "\n",
    "# CV用の配列を初期化\n",
    "oof_predictions = np.zeros((len(train), len(cfg.target_cols)))\n",
    "models = {}\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf.split(train, groups=train[\"scene\"])):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    # データセットの作成\n",
    "    train_dataset = CustomDataset(train[train_idx], cfg.data.img_root, transform=get_train_transform())\n",
    "    valid_dataset = CustomDataset(train[valid_idx], cfg.data.img_root, transform=get_valid_transform())\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.cnn.batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=cfg.cnn.batch_size, shuffle=False)\n",
    "\n",
    "    # モデル、損失関数、オプティマイザーの初期化\n",
    "    model = CustomModel(cfg.cnn).to(device)\n",
    "    criterion = nn.HuberLoss()\n",
    "    # criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.cnn.lr)\n",
    "    total_steps = len(train_loader) * cfg.cnn.num_epochs\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=total_steps * 0.1,\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    # 学習ループ\n",
    "    for epoch in range(cfg.cnn.num_epochs):\n",
    "        model.train()\n",
    "        for images, targets in tqdm(train_loader):\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # 検証\n",
    "        model.eval()\n",
    "        valid_losses = []\n",
    "        with torch.no_grad():\n",
    "            for images, targets in valid_loader:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, targets)\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        print(f\"Epoch {epoch + 1}, Valid Loss: {valid_loss:.4f}\")\n",
    "\n",
    "        # ベストモデルの保存\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f\"{cfg.data.results_path}/model_fold{fold}.pth\")\n",
    "\n",
    "    # ベストモデルでOOF予測\n",
    "    model.load_state_dict(torch.load(f\"{cfg.data.results_path}/model_fold{fold}.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    valid_dataset = CustomDataset(train[valid_idx], cfg.data.img_root, transform=get_valid_transform())\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=cfg.cnn.batch_size, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(valid_loader):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            start_idx = i * cfg.cnn.batch_size\n",
    "            end_idx = start_idx + outputs.shape[0]\n",
    "            oof_predictions[valid_idx[start_idx:end_idx]] = outputs.cpu().numpy()\n",
    "\n",
    "# CVスコアの計算（MAEの平均）\n",
    "mae_scores = []\n",
    "for i in range(len(cfg.target_cols)):\n",
    "    mae = np.mean(np.abs(oof_predictions[:, i] - train[cfg.target_cols[i]].to_numpy()))\n",
    "    mae_scores.append(mae)\n",
    "\n",
    "cv_score = np.mean(mae_scores)\n",
    "print(f\"CV Score: {cv_score:.4f}\")\n",
    "\n",
    "# oofを保存\n",
    "np.save(f\"{cfg.data.results_path}/oof_predictions.npy\", oof_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testの推論\n",
    "test_dataset = CustomDataset(test, cfg.data.img_root, transform=get_valid_transform(), is_train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.cnn.batch_size, shuffle=False)\n",
    "\n",
    "# 5fold分の予測値を格納する配列\n",
    "test_predictions = np.zeros((len(test), len(cfg.target_cols), cfg.n_splits))\n",
    "\n",
    "# 各foldのモデルで予測\n",
    "for fold in range(cfg.n_splits):\n",
    "    print(f\"Predicting using fold {fold + 1} model\")\n",
    "\n",
    "    # モデルの読み込み\n",
    "    model = CustomModel(cfg.cnn).to(device)\n",
    "    model.load_state_dict(torch.load(f\"{cfg.data.results_path}/model_fold{fold}.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    fold_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            fold_predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "    # バッチごとの予測を結合\n",
    "    fold_predictions = np.concatenate(fold_predictions, axis=0)\n",
    "    test_predictions[:, :, fold] = fold_predictions\n",
    "\n",
    "# 5fold分の予測値の平均を計算\n",
    "final_predictions = test_predictions.mean(axis=2)\n",
    "\n",
    "# submissionファイルの作成\n",
    "exprs = [pl.Series(final_predictions[:, i]).alias(cfg.target_cols[i]) for i in range(len(cfg.target_cols))]\n",
    "submission = sample_submission.with_columns(exprs)\n",
    "submission.write_csv(f\"{cfg.data.results_path}/submission.csv\")\n",
    "print(\"Submission file created!\")\n",
    "\n",
    "# 確認\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_predictionsを保存\n",
    "np.save(f\"{cfg.data.results_path}/final_predictions.npy\", final_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
