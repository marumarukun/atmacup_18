{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/atmacup_18/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import AutoProcessor, AutoTokenizer, Qwen2VLForConditionalGeneration\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "### Prompt\n",
    "\"Analyze three consecutive 128x64 resolution in-vehicle camera images (ordered by time) and provide essential driving conditions in JSON format. Based solely on the actual content of these images, describe the driving situation focusing on features that directly influence vehicle trajectory prediction.\n",
    "\n",
    "Your response should:\n",
    "1. Reflect the actual scene in the images, not repeat the example\n",
    "2. Consider the temporal changes across the three frames\n",
    "3. Strictly use only the specified options for each field\n",
    "4. Output in JSON format only\n",
    "\n",
    "### JSON Output Format\n",
    "{\n",
    "  \"road_type\": string,  // Options: [\n",
    "                        //   \"highway\",\n",
    "                        //   \"urban_arterial\",\n",
    "                        //   \"intersection\",\n",
    "                        //   \"merging_zone\",\n",
    "                        //   \"curve_section\"\n",
    "                        // ]\n",
    "\n",
    "  \"road_geometry\": {\n",
    "    \"curvature\": string,  // Options: [\"straight\", \"curve\"]\n",
    "    \"slope\": string,      // Options: [\"uphill\", \"downhill\", \"flat\"]\n",
    "    \"curve_direction\": string,  // Options: [\"left\", \"right\", \"none\"]\n",
    "    \"curve_sharpness\": string  // Options: [\"gentle\", \"sharp\", \"none\"]\n",
    "  },\n",
    "\n",
    "  \"lane_count\": number,  // Options: [1, 2, 3, \"multiple\"]\n",
    "\n",
    "  \"front_vehicle\": {\n",
    "    \"present\": boolean,  // Options: [true, false]\n",
    "    \"distance\": string,  // Options: [\"close\", \"medium\", \"far\"]\n",
    "    \"type\": string,      // Options: [\"large\", \"normal\", \"none\"]\n",
    "    \"relative_motion\": {\n",
    "      \"speed_difference\": string,  // Options: [\"approaching\", \"maintaining\", \"separating\"]\n",
    "      \"lateral_position\": string,  // Options: [\"center\", \"left_side\", \"right_side\"]\n",
    "      \"trajectory\": string         // Options: [\"stable\", \"changing_left\", \"changing_right\"]\n",
    "    }\n",
    "  },\n",
    "\n",
    "  \"visibility\": string,  // Options: [\"clear\", \"dark\", \"poor\"]\n",
    "  \"traffic_flow\": string  // Options: [\"stopped\", \"moving\", \"fast\"]\n",
    "}\n",
    "\n",
    "Return only the JSON output based on the actual scene in the provided sequence of images.\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messages containing multiple images and a text query\n",
    "\n",
    "id_name = \"ff8ac16fafde4aebe8e9632a0d382ef1_320\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": f\"../../data/input/atma18/images/{id_name}/image_t-1.0.png\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": f\"../../data/input/atma18/images/{id_name}/image_t-0.5.png\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": f\"../../data/input/atma18/images/{id_name}/image_t.png\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ],\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/atmacup_18/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['```json\\n{\\n  \"road_type\": \"highway\",\\n  \"road_geometry\": {\\n    \"curvature\": \"straight\",\\n    \"slope\": \"flat\",\\n    \"curve_direction\": \"none\",\\n    \"curve_sharpness\": \"none\"\\n  },\\n  \"lane_count\": 2,\\n  \"front_vehicle\": {\\n    \"present\": true,\\n    \"distance\": \"close\",\\n    \"type\": \"normal\",\\n    \"relative_motion\": {\\n      \"speed_difference\": \"approaching\",\\n      \"lateral_position\": \"center\",\\n      \"trajectory\": \"stable\"\\n    }\\n  },\\n  \"visibility\": \"clear\",\\n  \"traffic_flow\": \"moving\"\\n}\\n```']\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=256)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids, strict=False)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('```json\\n'\n",
      " '{\\n'\n",
      " '  \"road_type\": \"highway\",\\n'\n",
      " '  \"road_geometry\": {\\n'\n",
      " '    \"curvature\": \"straight\",\\n'\n",
      " '    \"slope\": \"flat\",\\n'\n",
      " '    \"curve_direction\": \"none\",\\n'\n",
      " '    \"curve_sharpness\": \"none\"\\n'\n",
      " '  },\\n'\n",
      " '  \"lane_count\": 2,\\n'\n",
      " '  \"front_vehicle\": {\\n'\n",
      " '    \"present\": true,\\n'\n",
      " '    \"distance\": \"close\",\\n'\n",
      " '    \"type\": \"normal\",\\n'\n",
      " '    \"relative_motion\": {\\n'\n",
      " '      \"speed_difference\": \"approaching\",\\n'\n",
      " '      \"lateral_position\": \"center\",\\n'\n",
      " '      \"trajectory\": \"stable\"\\n'\n",
      " '    }\\n'\n",
      " '  },\\n'\n",
      " '  \"visibility\": \"clear\",\\n'\n",
      " '  \"traffic_flow\": \"moving\"\\n'\n",
      " '}\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(output_text[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
